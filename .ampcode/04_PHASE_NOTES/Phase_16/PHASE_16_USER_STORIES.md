

# ‚≠ê **PHASE‚Äë16 USER STORIES (FINAL, AUTHORITATIVE)**
### *Asynchronous Job Queue + Persistent Job State + Worker Execution*

Each story is atomic, testable, and maps to one commit.

---

## **Story 1 ‚Äî Job Model + DB Migration**
**Commit 1 of 10**

### **Story**
As a backend engineer, I want a persistent job table so that job submissions can be tracked across restarts.

### **Acceptance Criteria**
- **Alembic initialized from scratch** ‚Äî no Alembic exists in the repo today
  - Run `alembic init server/app/migrations`
  - Configure `env.py` to import the shared SQLAlchemy Base
  - Configure `alembic.ini` with `sqlalchemy.url = duckdb:///data/foregsyte.duckdb`
- **`server/app/core/database.py` created** ‚Äî no SQLAlchemy session management exists today
  - Shared `Base = declarative_base()`
  - `engine` using `duckdb_engine` dialect pointing to `duckdb:///data/foregsyte.duckdb`
  - `SessionLocal` sessionmaker
  - **Must use the existing DuckDB instance** ‚Äî the project already uses DuckDB, do not introduce a second database engine
- SQLAlchemy model created at `server/app/models/job.py`
  - Must use the **shared Base from `app/core/database.py`** ‚Äî not its own `declarative_base()`
- First Alembic migration created for `jobs` table
- Job table fields:

| Field | Type | Notes |
|-------|------|-------|
| job_id | UUID (PK) | generated server‚Äëside |
| status | enum | pending, running, completed, failed |
| created_at | timestamp | auto |
| updated_at | timestamp | auto |
| pipeline_id | string | required |
| input_path | string | path in object storage |
| output_path | string | path to results JSON |
| error_message | string | nullable |

- Import pattern: `from app.models.job import Job`
- Dependency: `duckdb_engine` must be added to project dependencies

### **Tests**
- `tests/models/test_job.py`
- Unit tests use **mocks only**
- Integration tests use **in‚Äëmemory DuckDB** (`duckdb:///:memory:`)
- Integration tests must run Alembic migrations against in‚Äëmemory DuckDB
- 100% passing

### üö´ **OUT OF SCOPE**
- [ ] `frame_stride` or `max_frames` columns ‚Äî Phase‚Äë15 synchronous batch concepts, not Phase‚Äë16 async job concepts
- [ ] `progress` column ‚Äî progress is derived from status, not stored
- [ ] `worker_id` column ‚Äî distributed worker tracking is Phase 18+
- [ ] `priority` column ‚Äî job prioritization is future scope
- [ ] Queue logic in this commit
- [ ] Worker logic in this commit
- [ ] Manual table creation ‚Äî use Alembic migration only
- [ ] Status values beyond the four allowed (pending, running, completed, failed)
- [ ] Phase‚Äënamed file (`phase16_job.py`) ‚Äî use functional name `job.py`
- [ ] Creating a separate `declarative_base()` in `job.py` ‚Äî must use shared Base from `app/core/database.py`
- [ ] `AsyncSession` or async SQLAlchemy ‚Äî codebase is synchronous
- [ ] Pre‚Äëcommit hook for migration enforcement
- [ ] SQLite ‚Äî project uses DuckDB, do not introduce a second database engine
- [ ] PostgreSQL or any external database server

---

## **Story 2 ‚Äî Object Storage Adapter**
**Commit 2 of 10**

### **Story**
As a contributor, I want a storage abstraction so that MP4 files and results JSON can be saved and retrieved independently of local disk.

### **Acceptance Criteria**
- `StorageService` interface at `server/app/services/storage/base.py`
- Local filesystem implementation at `server/app/services/storage/local_storage.py`
- Deterministic paths:

```
./data/video_jobs/{job_id}.mp4
./data/video_jobs/{job_id}_results.json
```

- Methods:
  - `save_file(src, dest_path)`
  - `load_file(path)`
  - `delete_file(path)`

### **Tests**
- `tests/services/storage/test_local_storage.py`
- Unit tests use temp directories

### üö´ **OUT OF SCOPE**
- [ ] S3 storage ‚Äî local filesystem only in Phase‚Äë16
- [ ] Cloud storage adapters of any kind
- [ ] Compression or encoding logic
- [ ] File streaming or chunked uploads
- [ ] File expiry or TTL logic
- [ ] Directory listing or search capabilities
- [ ] Storing results in the database ‚Äî results go to object storage as JSON files
- [ ] Hardcoded absolute paths ‚Äî use relative, configurable paths
- [ ] Phase‚Äëprefixed filenames (`phase16_storage.py`)

---

## **Story 3 ‚Äî Queue Adapter**
**Commit 3 of 10**

### **Story**
As a contributor, I want a queue abstraction so that job IDs can be pushed and popped without binding to a specific backend.

### **Acceptance Criteria**
- `QueueService` interface at `server/app/services/queue/base.py`
- In‚Äëmemory implementation at `server/app/services/queue/memory_queue.py`
- Backend: Python `queue.Queue`
- Methods:
  - `enqueue(job_id)`
  - `dequeue()`
  - `size()`

### **Tests**
- `tests/services/queue/test_memory_queue.py`

### üö´ **OUT OF SCOPE**
- [ ] Redis queue ‚Äî in‚Äëmemory only in Phase‚Äë16
- [ ] RabbitMQ, Celery, or any external queue backend
- [ ] Metadata in the queue payload ‚Äî strictly `{job_id}` only
- [ ] Priority queue logic
- [ ] Dead‚Äëletter queue logic
- [ ] Queue persistence or durability
- [ ] Queue acknowledgment or visibility timeout
- [ ] Batch enqueue/dequeue operations
- [ ] Queue monitoring or metrics endpoints
- [ ] Job metadata (status, paths, pipeline_id) in the queue message

---

## **Story 4 ‚Äî Job Submission Endpoint**
**Commit 4 of 10**

### **Story**
As an API consumer, I want to submit a video for processing and receive a job_id.

### **Acceptance Criteria**
- Route: `POST /video/submit`
- Route file at **`server/app/api_routes/routes/job_submit.py`** ‚Äî matches existing Phase‚Äë15 location pattern (`api_routes/routes/`)
- Uses FastAPI `UploadFile`
- MP4 validation via **magic bytes** (`ftyp`)
- Saves file via StorageService
- Creates job row with:
  - job_id
  - pipeline_id
  - input_path
  - status = pending
- Enqueues job_id
- Returns:

```json
{ "job_id": "<uuid>" }
```

### **Tests**
- `tests/api/test_job_submit.py`
- Uses static fixture: `tests/fixtures/tiny.mp4`
- API tests use **in‚Äëmemory DuckDB** (`duckdb:///:memory:`) via TestClient fixture
- SQLAlchemy session is **synchronous** (FastAPI routes can be async, SQLAlchemy stays sync)

### üö´ **OUT OF SCOPE**
- [ ] MP4 validation by file extension ‚Äî use magic bytes only
- [ ] MP4 validation with ffprobe or ffmpeg
- [ ] Returning status, progress, or estimated completion time in the submit response ‚Äî only `{job_id}`
- [ ] Synchronous processing ‚Äî submission must be fully async
- [ ] Batch submissions (multiple files per request)
- [ ] Non‚ÄëMP4 video formats (MOV, AVI, etc.) in Phase‚Äë16
- [ ] Authentication or authorization logic
- [ ] Rate limiting
- [ ] File size limits beyond reasonable defaults
- [ ] Blocking the HTTP response waiting for worker pickup
- [ ] Creating routes under `server/app/api/routes/` ‚Äî use `server/app/api_routes/routes/` to match existing codebase

---

## **Story 5 ‚Äî Worker Skeleton**
**Commit 5 of 10**

### **Story**
As a worker engineer, I want a worker loop that pulls job IDs and marks jobs as running.

### **Acceptance Criteria**
- Worker class at `server/app/workers/worker.py`
- Entry point at `server/app/workers/worker_runner.py`
- Methods:
  - `run_once()`
  - `run_forever()`
- Behavior:
  - dequeue job_id
  - load job row
  - mark status ‚Üí running
  - handle empty queue with sleep/backoff
  - handle SIGINT/SIGTERM gracefully

### **Tests**
- `tests/workers/test_job_worker.py`
- Unit tests use **mocks** for queue and DB
- Integration tests use **in‚Äëmemory DuckDB** (`duckdb:///:memory:`)

### üö´ **OUT OF SCOPE**
- [ ] Pipeline execution in this commit ‚Äî skeleton only, no processing
- [ ] Multi‚Äëthreaded or multi‚Äëprocess worker pools
- [ ] Distributed worker coordination
- [ ] GPU scheduling or resource allocation
- [ ] Worker registration or discovery
- [ ] Health check endpoints for the worker
- [ ] Auto‚Äëscaling logic
- [ ] Docker/systemd/supervisor configuration ‚Äî plain Python entry point only
- [ ] Skipping signal handling ‚Äî SIGINT and SIGTERM must be handled gracefully
- [ ] Busy‚Äëlooping without sleep/backoff when queue is empty

---

## **Story 6 ‚Äî Worker Executes Phase‚Äë15 Pipeline**
**Commit 6 of 10**

### **Story**
As a worker engineer, I want the worker to run the Phase‚Äë15 VideoFilePipelineService on the submitted MP4.

### **Acceptance Criteria**
- Worker loads MP4 via StorageService
- Calls `VideoFilePipelineService.run_on_file()`
- Saves results JSON to:

```
./data/video_jobs/{job_id}_results.json
```

- Updates job row:
  - output_path
  - status ‚Üí completed
- On exception:
  - status ‚Üí failed
  - error_message stored

### **Tests**
- `tests/workers/test_worker_pipeline.py`
- Uses both fixtures:
  - Static: `tests/fixtures/tiny.mp4` (fast, deterministic)
  - Dynamic: `tests/fixtures/make_tiny_mp4.py` (regenerate if corrupted or missing)
- Worker tests use **in‚Äëmemory DuckDB** (`duckdb:///:memory:`)

### üö´ **OUT OF SCOPE**
- [ ] Creating a new pipeline service ‚Äî reuse Phase‚Äë15 `VideoFilePipelineService` exactly
- [ ] Duplicating pipeline execution logic
- [ ] Frame‚Äëlevel progress callbacks or reporting
- [ ] Real‚Äëtime streaming of results
- [ ] Storing results in the database ‚Äî store as JSON in object storage only
- [ ] Retry logic in this commit ‚Äî simple success/fail only
- [ ] Timeout logic for pipeline execution
- [ ] Swallowing exceptions silently ‚Äî always set error_message on failure
- [ ] Leaving job in `running` status if an exception occurs ‚Äî must transition to `failed`

---

## **Story 7 ‚Äî Job Status Endpoint**
**Commit 7 of 10**

### **Story**
As an API consumer, I want to check the status of a job.

### **Acceptance Criteria**
- Route: `GET /video/status/{job_id}`
- Route file at **`server/app/api_routes/routes/job_status.py`**
- Returns:

```json
{
  "job_id": "...",
  "status": "...",
  "progress": 0 | 0.5 | 1.0,
  "created_at": "...",
  "updated_at": "..."
}
```

- Progress rules:
  - pending ‚Üí 0
  - running ‚Üí 0.5
  - completed/failed ‚Üí 1.0
- 404 if job not found

### **Tests**
- `tests/api/test_job_status.py`
- API tests use **in‚Äëmemory DuckDB** (`duckdb:///:memory:`) via TestClient fixture

### üö´ **OUT OF SCOPE**
- [ ] Frame‚Äëlevel progress (e.g., "frame 47 of 120") ‚Äî coarse progress only (0, 0.5, 1.0)
- [ ] Estimated completion time
- [ ] Processing speed or performance metrics
- [ ] WebSocket push notifications
- [ ] Server‚ÄëSent Events (SSE)
- [ ] Long‚Äëpolling
- [ ] Storing progress as a database column ‚Äî derive it from status
- [ ] Returning partial results in the status response
- [ ] Queue position information

---

## **Story 8 ‚Äî Job Results Endpoint**
**Commit 8 of 10**

### **Story**
As an API consumer, I want to retrieve results once the job is completed.

### **Acceptance Criteria**
- Route: `GET /video/results/{job_id}`
- Route file at **`server/app/api_routes/routes/job_results.py`**
- Returns:

```json
{
  "job_id": "...",
  "results": [...]
}
```

- 404 if:
  - job not found
  - job not completed

### **Tests**
- `tests/api/test_job_results.py`
- API tests use **in‚Äëmemory DuckDB** (`duckdb:///:memory:`) via TestClient fixture

### üö´ **OUT OF SCOPE**
- [ ] Partial results for in‚Äëprogress jobs
- [ ] Results for failed jobs ‚Äî 404 only
- [ ] Pagination of results
- [ ] Result filtering or search
- [ ] Result download as file (e.g., CSV export)
- [ ] Processing time or performance metrics in the results response
- [ ] Changing the Phase‚Äë15 output format ‚Äî results must match exactly
- [ ] Caching results in memory ‚Äî always load from object storage
- [ ] Result expiry or TTL

---

## **Story 9 ‚Äî Governance + CI Enforcement**
**Commit 9 of 10**

### **Story**
As a release manager, I want governance rules preventing Phase‚Äë17 concepts from leaking into Phase‚Äë16.

### **Acceptance Criteria**
- Create:
  - `forbidden_vocabulary_phase16.yaml`
  - `validate_phase16_path.py`
  - `.github/workflows/phase16_validation.yml`
- Forbidden terms:
  - websocket
  - streaming
  - gpu_schedule
  - distributed
  - gpu_worker
  - sse (server‚Äësent events)
  - real_time
- Smoke test updated:
  - submit ‚Üí status ‚Üí results

### **Tests**
- Governance tests must pass
- CI must block violations

### üö´ **OUT OF SCOPE**
- [ ] Skipping forbidden vocabulary scanning ‚Äî must run on every PR
- [ ] Allowing Phase‚Äë17 vocabulary in functional code (WebSocket, streaming, SSE, GPU scheduling, distributed)
- [ ] Phase‚Äënamed files in functional directories (e.g., `phase16_worker.py` in `server/app/`)
- [ ] Governance documentation in functional directories ‚Äî keep in `.ampcode/04_PHASE_NOTES/Phase_16/`
- [ ] Making governance checks optional or soft‚Äëfail ‚Äî must block merge
- [ ] Governance exceptions or bypass mechanisms
- [ ] Skipping the smoke test in CI
- [ ] Allowing pre‚Äëcommit hooks to be disabled
- [ ] Pre‚Äëcommit hook for Alembic migration enforcement ‚Äî not required

---

## **Story 10 ‚Äî Documentation + Rollback Plan**
**Commit 10 of 10**

### **Story**
As a new contributor, I want complete Phase‚Äë16 documentation so I can understand, contribute to, and if necessary roll back the entire phase.

### **Acceptance Criteria**
- Create/update:
  - Overview
  - Architecture
  - Endpoints
  - Migration Guide
  - Rollback Plan
  - Contributor Exam
  - Release Notes
- Rollback plan must be executable and must include:
  - Reverting the Alembic migration (dropping `jobs` table from DuckDB)
  - Removing `server/app/core/database.py` if it was created solely for Phase‚Äë16
  - Removing all Phase‚Äë16 route files from `server/app/api_routes/routes/`
  - Removing `server/app/workers/`
  - Removing `server/app/services/storage/`
  - Removing `server/app/services/queue/`
  - Removing `server/app/models/job.py`
  - Removing `duckdb_engine` from project dependencies if no other code uses it
- All links verified

### **Tests**
- All documentation files exist
- All cross‚Äëreferences valid

### üö´ **OUT OF SCOPE**
- [ ] Leaving the rollback plan as a stub ‚Äî must list every file to remove and every migration to revert
- [ ] Referencing Phase‚Äë17 features as if they exist
- [ ] Speculative architecture for future phases
- [ ] Skipping the contributor exam ‚Äî must be complete with answer key
- [ ] Broken cross‚Äëreference links between documents
- [ ] Documenting features that were not implemented
- [ ] TODO items ‚Äî Phase‚Äë16 documentation must be complete as‚Äëis
- [ ] Mixing Phase‚Äë16 docs with earlier phase docs ‚Äî keep in `Phase_16/` directory only

---

# ‚≠ê **GLOBAL OUT OF SCOPE ‚Äî APPLIES TO ALL STORIES**

These rules apply to **every single commit** across Phase‚Äë16:

| # | Item | Consequence |
|---|------|-------------|
| G1 | WebSockets | PR rejected, CI failure |
| G2 | Server‚ÄëSent Events (SSE) | PR rejected, CI failure |
| G3 | Real‚Äëtime streaming | PR rejected, CI failure |
| G4 | GPU scheduling | PR rejected, CI failure |
| G5 | Distributed worker coordination | PR rejected, CI failure |
| G6 | Multi‚Äëpipeline DAG orchestration | PR rejected, CI failure |
| G7 | Frame‚Äëlevel progress tracking | PR rejected, code review rejection |
| G8 | Metadata in queue payloads beyond `{job_id}` | Schema violation |
| G9 | Job status values beyond the four allowed | Schema violation |
| G10 | Phase‚Äëprefixed filenames in functional directories | Governance violation |
| G11 | Redis, RabbitMQ, Celery, or external queue backends | Architecture violation |
| G12 | S3 or cloud storage | Architecture violation |
| G13 | Authentication, authorization, or rate limiting | Scope violation |
| G14 | Job cancellation, job chaining, or job dependencies | Scope violation |
| G15 | Worker auto‚Äëscaling or load balancing | Scope violation |
| G16 | Changing the Phase‚Äë15 pipeline output format | Regression violation |
| G17 | Skipping tests ‚Äî every commit must have passing tests | Quality gate failure |
| G18 | Committing without running governance validator | Process violation |
| G19 | Docker, systemd, or container orchestration | Scope violation |
| G20 | Monitoring, alerting, or observability infrastructure | Scope violation |
| G21 | `AsyncSession` or async SQLAlchemy ‚Äî codebase is synchronous | Architecture violation |
| G22 | Creating routes under `api/routes/` instead of `api_routes/routes/` | Placement violation |
| G23 | Creating a separate `declarative_base()` per model file | Architecture violation |
| G24 | SQLite ‚Äî project uses DuckDB, do not introduce a second database engine | Architecture violation |
| G25 | PostgreSQL or any external database server | Architecture violation |

---

# ‚≠ê **KEY DECISIONS LOCKED BY Q&A**

| Decision | Answer | Affects |
|----------|--------|---------|
| Alembic exists? | **No ‚Äî create from scratch in Commit 1** | Story 1 |
| SQLAlchemy exists? | **No ‚Äî create `app/core/database.py` in Commit 1** | Story 1 |
| Database engine? | **DuckDB via `duckdb_engine`** ‚Äî project already uses DuckDB, do not introduce SQLite | Story 1, all stories |
| DB file path? | **`data/foregsyte.duckdb`** ‚Äî existing DuckDB instance | Story 1 |
| Alembic URL? | **`duckdb:///data/foregsyte.duckdb`** | Story 1 |
| Shared Base? | **Yes ‚Äî single `declarative_base()` in `app/core/database.py`** | Story 1, all models |
| Session type? | **Synchronous `SessionLocal`** ‚Äî no `AsyncSession` | All stories |
| Test DB? | **In‚Äëmemory DuckDB (`duckdb:///:memory:`)** ‚Äî not SQLite | All test stories |
| API route path? | **`server/app/api_routes/routes/`** ‚Äî matches Phase‚Äë15 | Stories 4, 7, 8 |
| Test DB strategy? | **Mocks for unit, in‚Äëmemory DuckDB for integration** | All test stories |
| Test fixtures? | **Both static `tiny.mp4` and dynamic `make_tiny_mp4.py`** | Stories 4, 6 |
| Pre‚Äëcommit migration hook? | **No ‚Äî not required** | Story 9 |
| Import pattern? | **`from app.models.job import Job`** | All stories |